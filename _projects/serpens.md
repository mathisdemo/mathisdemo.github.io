---
layout: page
title: serpens
description: No description available
full_name: mathisdemo/serpens
img: assets/img/projects/serpens/main.png
importance: 1
git: https://github.com/mathisdemo/serpens
github: https://github.com/mathisdemo/serpens
category: Computer Science
subcategory: Software Development
---

Ce projet met en Å“uvre un **agent dâ€™apprentissage par renforcement (Q-Learning)** dans un environnement simplifiÃ© de type *Snake*.  
Lâ€™agent apprend Ã  se dÃ©placer sur une carte 15Ã—15 pour atteindre une position cible (la pomme ğŸ) sans sortir des limites de la carte.  
Une visualisation graphique en **Pygame** permet dâ€™observer les dÃ©placements de lâ€™agent aprÃ¨s lâ€™entraÃ®nement.

---

## ğŸ“ Structure du projet


```

â”œâ”€â”€ main.py                # Script principal : logique Q-learning + visualisation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sprites/
â”‚   â”‚   â”œâ”€â”€ snake.png      # Sprite du serpent (agent)
â”‚   â”‚   â””â”€â”€ appel.png      # Sprite de la pomme
â”‚   â””â”€â”€ map/
â”‚       â””â”€â”€ map_snake.tmx  # Carte du jeu (format Tiled Map Editor)
â”œâ”€â”€ README.md              # Ce fichier

````

---

## âš™ï¸ Fonctionnement gÃ©nÃ©ral


### ğŸ§  1. EntraÃ®nement (Q-Learning)


Lâ€™agent, appelÃ© `SubjectOfTraining`, apprend Ã  atteindre la cible `(5, 5)` sur une grille de taille 15Ã—15 Ã  lâ€™aide du **Q-learning**.

#### ParamÃ¨tres dâ€™apprentissage :


| ParamÃ¨tre | Symbole | Valeur par dÃ©faut | RÃ´le |
|------------|----------|------------------|------|
| Taux dâ€™apprentissage | Î± | 0.5 | PondÃ¨re la mise Ã  jour du Q-table |
| Facteur de rÃ©compense future | Î³ | 0.95 | Importance des rÃ©compenses futures |
| Taux dâ€™exploration | Îµ | 0.1 (dÃ©croissant) | ProbabilitÃ© de choisir une action alÃ©atoire |
| Nombre dâ€™Ã©pisodes | N | 1000 (par dÃ©faut) | Nombre de parties dâ€™entraÃ®nement |

#### RÃ©compenses :


- ğŸ **Atteindre la pomme** : `+10`
- ğŸš« **Sortir de la carte** : `-10`
- ğŸ•’ **Chaque pas** : `-0.05`

Lâ€™objectif est dâ€™apprendre une politique (table Q) qui maximise la rÃ©compense totale.

---

### ğŸ® 2. Visualisation (Pygame)


AprÃ¨s lâ€™entraÃ®nement, le script peut afficher une **animation** dâ€™un Ã©pisode particulier pour visualiser la trajectoire de lâ€™agent.

Lâ€™animation affiche :
- La carte (`map_snake.tmx`)
- Le serpent (agent)
- La pomme (objectif fixe Ã  `(5, 5)`)

---

## ğŸ§© Classes principales


| Classe | RÃ´le |
|--------|------|
| **`SubjectOfTraining`** | DÃ©finit lâ€™agent (position, mouvement, rÃ©initialisation) |
| **`MasterOfTraining`** | ImplÃ©mente le Q-learning (choix dâ€™action, mise Ã  jour de la Q-table, calcul des rÃ©compenses) |
| **`EnvironnementOfTrainning`** | DÃ©finit les dimensions de la grille |
| **`RunProgramTraining`** | GÃ¨re la boucle dâ€™entraÃ®nement complÃ¨te |
| **`PrintOneEpisode`** | Affiche un Ã©pisode en Pygame |

---

## ğŸš€ Lancer le projet


### 1. Installer les dÃ©pendances


Assure-toi dâ€™avoir Python â‰¥ 3.8 puis installe les librairies nÃ©cessaires :

```bash
pip install pygame pytmx numpy
````

### 2. Lancer lâ€™entraÃ®nement



Depuis le dossier du projet :

```bash
python main.py
```

### 3. Observer le rÃ©sultat



Ã€ la fin de lâ€™entraÃ®nement, Pygame sâ€™ouvre et affiche le dÃ©placement du serpent pour un Ã©pisode donnÃ© (par dÃ©faut : le 999áµ‰).

---

## ğŸ§  Concepts utilisÃ©s


* **Q-Learning** : apprentissage tabulaire basÃ© sur lâ€™Ã©quation de Bellman
  ( Q(s,a) = (1 - lpha) Q(s,a) + lpha [r + \gamma \max_{a'} Q(s',a')] )
* **Exploration/Exploitation** : Ã©quilibre entre tester de nouvelles actions (Îµ-greedy) et exploiter ce que lâ€™on a dÃ©jÃ  appris.
* **RÃ©compense diffÃ©rÃ©e** : lâ€™agent apprend Ã  anticiper les rÃ©compenses Ã  long terme.

---

## ğŸ“Š AmÃ©liorations possibles


* âœ… Ajout de **graphique matplotlib** pour suivre lâ€™Ã©volution des rÃ©compenses par Ã©pisode
* âœ… Sauvegarde/chargement de la Q-table (`numpy.save` / `numpy.load`)
* âœ… Ajout de **collision avec soi-mÃªme** (snake complet)
* âœ… Variation de la position de la pomme pour un apprentissage plus gÃ©nÃ©ral
* âœ… Passage Ã  une version **Deep Q-Learning (DQN)** avec un rÃ©seau de neurones (PyTorch ou TensorFlow)

---

## ğŸ’¡ Exemple de rÃ©sultat attendu (conceptuellement)


AprÃ¨s plusieurs milliers dâ€™Ã©pisodes :

* Le serpent apprend Ã  se diriger vers `(5, 5)` rapidement.
* Les Ã©pisodes terminent plus vite et avec des rÃ©compenses plus Ã©levÃ©es.
* La Q-table contient les meilleures actions pour chaque position.

---

## ğŸ§‘â€ğŸ’» Auteur


**Projet de dÃ©monstration de Q-Learning avec Pygame**
DÃ©veloppÃ© pour lâ€™Ã©tude de lâ€™apprentissage par renforcement sur des environnements simples en grille.

---

